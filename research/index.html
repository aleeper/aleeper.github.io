<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Adam E. Leeper - Research</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Adam E. Leeper</div>
<div class="menu-item"><a href="../index.html">Home</a></div>
<div class="menu-item"><a href="../biography.html">Biography</a></div>
<div class="menu-item"><a href="../teaching.html">Teaching</a></div>
<div class="menu-item"><a href="../publications.html">Publications</a></div>
<div class="menu-category">Projects</div>
<div class="menu-item"><a href="../research/index.html" class="current">Research</a></div>
<div class="menu-item"><a href="http://mgview.github.io/mgview/">Simulation</a></div>
<div class="menu-category">Personal</div>
<div class="menu-item"><a href="../music/index.html">Music</a></div>
<div class="menu-item"><a href="../links.html">Links</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Adam E. Leeper - Research</h1>
</div>
<p>Research conducted in connection with the Stanford University
<a href="http://sr.stanford.edu">Salisbury BioRobotics Laboratory</a> and Willow Garage.</p>
<h2>Ph.D. Thesis</h2>
<p>[ <a href="https://s3-us-west-1.amazonaws.com/adamleeper.com/papers/leeper_thesis_2013.pdf">pdf</a> ] <br />
Abstract snippet:
This dissertation presents methods for robot teleoperation, or equivalently, human-in-the-
loop robotics. Human-in-the loop systems have the potential to handle complex tasks by
combining the cognitive skills of a human operator with autonomous tools and behaviors.
Along these lines, we present novel methods in grasp planning, haptic (force-feedback)
rendering, and robot control which allow synergy in interaction between a human operator
and a robot. We describe the interfaces that employ these algorithms, and validate them
through user experiments. Our goal is to see robot technologies make a bigger impact in
peoples&rsquo; everyday lives, getting robots out of the laboratory and factory, and into homes,
offices, and other <i>unstructured</i> human spaces.</p>
<h2>Mobile Manipulation</h2>
<table class="imgtable"><tr><td>
<img src="images/coffee_robot.jpg" alt="A Robot Autonomously Buying Coffee" width="400px" height="200px" />&nbsp;</td>
<td align="left"><p>Robots seldom leave the factory or the research lab.
This project,
<a href="http://spectrum.ieee.org/automaton/robotics/robotics-software/pr2-robot-fetches-cup-of-coffee">featured in IEEE Spectrum</a>,
is about getting robots to do things in unmodified human environments.
As a first step, the robot was programmed to autonomously go from the 1st
floor of the building to the coffee shop on the 3rd floor, interact with the
staff to purchase a drink, and deliver the drink to a thirsty programmer
back in the lab.</p>
</td></tr></table>
<ul>
<li><p>[ <a href="https://s3-us-west-1.amazonaws.com/adamleeper.com/papers/coffee_icra2013.pdf">pdf</a> ]
[ <a href="http://www.youtube.com/watch?v=SLGGEELcQoM">YouTube Video</a> ]
Anthony Pratkanis, Adam Eric Leeper, Kenneth Salisbury.
Replacing the Office Intern: An Autonomous Coffee Run with a Mobile Manipulator.
ICRA, May 2013, Germany.</p>
</li>
</ul>
<h2>Assisted Teleoperation</h2>
<table class="imgtable"><tr><td>
<img src="images/cat_teaser.jpg" alt="An Interface for Assisted Teleoperation" width="400px" height="200px" />&nbsp;</td>
<td align="left"><p>This project combines elements of telerobotics, haptics, perception, planning, and control
to create a flexible, powerful tool to assist teleoperators.
The main thesis of this work is that there is no single best method for arm teleoperation
when the goal is to work quickly while avoiding bad collisions.
Instead, some sub-tasks will benefit from a responsive, predictable, &ldquo;dumb&rdquo; controller,
while in other cases a global motion planner will be the fastest way to accomplish a subtask.</p>
</td></tr></table>
<ul>
<li><p>[ <a href="https://s3-us-west-1.amazonaws.com/adamleeper.com/papers/2013_Humanoids_leeper-hsiao-ciocarlie-sucan-salisbury.pdf">pdf</a> ]
[ <a href="https://www.youtube.com/watch?v=JR_hvPWtHoM">YouTube Video</a> ]
<b>Adam Leeper</b>, Kaijen Hsiao, Matei Ciocarlie, Ioan Sucan, and Kenneth Salisbury.
Methods for Collision-Free Arm Teleoperation in Clutter Using Constraints from 3D Sensor Data.
2013 IEEE-RAS International Conference on Humanoid Robots. October, 2013. Atlanata, Georgia.</p>
</li>
</ul>
<h2>Human-in-the-Loop Robotics</h2>
<table class="imgtable"><tr><td>
<img src="images/hri_teaser2.jpg" alt="An Interface For Human-in-the-Loop Manipulation" width="400px" height="200px" />&nbsp;</td>
<td align="left"><p>This project seeks to create efficient tools for directing a robot to perform manipulation tasks
through mixed-initiative or shared-autonomy strategies.
While initially we targeted remote manipulation scenarios (where the robot may be in another room, or another state),
we have also found applications in helping a disabled person to be able to manipulate his own nearby environment.
These tools can be used as part of a larger human-in-the-loop robotic task system, where the robot is able to ask a human operator for help when it encounters an unusual situation. Such systems may be able to bring useful robot technologies to market long before a fully autonomous robot is reliable enough for wide-scale deployment.</p>
</td></tr></table>
<ul>
<li><p>[ <a href="https://s3-us-west-1.amazonaws.com/adamleeper.com/papers/2012_HRI_leeper-hsiao-ciocarlie-takayama-gossow.pdf">pdf</a> ]
[ <a href="https://s3-us-west-1.amazonaws.com/adamleeper.com/videos/2012_HRI_remote_grasping.mp4">video (11MB)</a> ]
A. Leeper, K. Hsiao, M. Ciocarlie, L. Takayama, D. Gossow.
Strategies for Human-in-the-Loop Robotic Grasping.
HRI, March 2012, Boston, MA.</p>
</li>
<li><p>[ <a href="http://www.willowgarage.com/sites/default/files/rfh_im.pdf">pdf</a> ]
[ <a href="https://s3-us-west-1.amazonaws.com/adamleeper.com/videos/2012_IROS_rfh.mp4">video (10MB)</a> ]
M. Ciocarlie, K. Hsiao, A. Leeper, D. Gossow.
Mobile Manipulation Through An Assistive Home Robot.
IROS, September 2012, Vilamoura, Algarve, Portugal.</p>
</li>
</ul>
<h2>Constraint-based Haptic Rendering of Point Clouds</h2>
<h3>3-DOF Rendering</h3>
<table class="imgtable"><tr><td>
<img src="images/sphere_and_bunny.jpg" alt="Unordered Point Clouds" width="400px" height="200px" />&nbsp;</td>
<td align="left"><p>We present a constraint-based haptic rendering
algorithm for arbitrary point cloud data. With the recent
proliferation of low-cost range sensors, 3D point cloud data
is readily available at high update rates. Challenges in haptic
rendering of this data arise due to noise and poorly defined
surface composition. We propose that point cloud data can be
rendered as an implicit surface, which can be formulated to be
mathematically smooth and efficient for computing interaction
forces. This method is resistant to sensor noise, makes no
assumptions about surface connectivity or orientation, and has
fast pre-processing suitable for streaming data from a sensor.</p>
</td></tr></table>
<ul>
<li><p>[ <a href="https://s3-us-west-1.amazonaws.com/adamleeper.com/papers/2012_ICRA_leeper-chan-salisbury.pdf">pdf</a> ]
[ <a href="https://s3-us-west-1.amazonaws.com/adamleeper.com/videos/2012_ICRA_3DOF_point_haptics.mp4">video (5MB)</a> ]
A. Leeper, S. Chan, K. Salisbury.
Point Clouds Can Be Represented as Implicit Surfaces for Constraint-Based Haptic Rendering.
ICRA, May 2012, St Paul, MN.</p>
</li>
</ul>
<h3>6-DOF Rendering</h3>
<table class="imgtable"><tr><td>
<img src="images/haptics_teaser.jpg" alt="6-DOF Haptic Interaction with Point Clouds" width="400px" height="200px" />&nbsp;</td>
<td align="left"><p>We present an efficient 6-DOF haptic algorithm for rendering
interaction forces between a rigid proxy object and a set of unordered
point data. We thus explore the use of haptic feedback for remotely
supervised robots performing grasping tasks. The robot captures
the geometry of a remote environment (as a cloud of 3D points) at
run-time using a depth camera or laser scanner. An operator then
uses a haptic device to position a virtual model of the robot gripper
(the haptic proxy), specifying a desired grasp pose to be executed by
the robot. The haptic algorithm enforces a proxy pose that is non-
colliding with the observable environment, and provides both force
and torque feedback to the operator. Once the operator confirms the
desired gripper pose, the robot computes a collision-free arm
trajectory and executes the specified grasp. We apply this method for
grasping a wide range of objects, previously unseen by the robot,
from highly cluttered scenes typical of human environments.




</p>
</td></tr></table>
<ul>
<li><p>[ <a href="https://s3-us-west-1.amazonaws.com/adamleeper.com/papers/2012_HS_leeper-chan-hsiao-ciocarlie-salisbury.pdf">pdf</a> ]
[ <a href="http://www.youtube.com/watch?v=evmZexa8LFE">youtube</a> ]
[ <a href="https://s3-us-west-1.amazonaws.com/adamleeper.com/videos/2012_HS_6DOF_point_haptics.mp4">video (11MB)</a> ]
A. Leeper, S. Chan, K. Hsiao, M. Ciocarlie, K. Salisbury.
Constraint-Based Haptic Rendering of Point Data for Teleoperated Robotic Grasping.
Haptics Symposium, March 2012, Vancouver, BC, Canada.</p>
</li>
</ul>
<h2>Assisted Teleoperation</h2>
<p>The overall goal of this research was to investigate methods to augment remote teleoperation of a robot.
We approached this by employing an end-effector mounted stereo camera, and by demonstrating an algorithm for automated grasp adjustment.</p>
<p>Portions of this work are published in: <br /></p>
<ul>
<li><p>[ <a href="https://s3-us-west-1.amazonaws.com/adamleeper.com/papers/2010_ISER_leeper-hsiao-chu-salisbury.pdf">pdf</a> ]
[ <a href="http://www.youtube.com/watch?v=58wmsHy-Nw0">youtube</a> ]
A. Leeper, K. Hsiao, E. Chu, and K. Salisbury.
Using Near-Field Stereo Vision for Robotic Grasping in Cluttered Environments.
Proceedings of ISER 2010. Dec, 2010.</p>
</li>
</ul>
<p>Another video summarizing portions of this work is on youtube <a href="http://www.youtube.com/watch?v=xY3cnMdjbow">here</a>.</p>
<h3>Gripper Stereo</h3>
<table class="imgtable"><tr><td>
<img src="images/gripper.jpg" alt="Gripper Stereo Cameras" width="150px" height="150px" />&nbsp;</td>
<td align="left"><p>We prototyped a variety of miniature stereo camera sensors designed for use on a robot end-effector.
This provides 3D information needed for pose estimates along with shape, color, texture, and other local surface information.</p>
</td></tr></table>
<h3>Grasp Selection Using 3D Point Cloud Features</h3>
<table class="imgtable"><tr><td>
<img src="images/adjust3.png" alt="Grasp Adjust" width="200px" height="150px" />&nbsp;</td>
<td align="left"><p>[ <a href="https://s3-us-west-1.amazonaws.com/adamleeper.com/videos/2010_ISER_grasp_adjust.mp4">video (5MB)</a> ]
To demonstrate the utility of the above sensor we created an algorithm for automatically finding or adjusting grasp poses.
The algorithm attempts to optimize the value of a grasp position by evaluating features of the 3D data.
The grasp adjustment algorithm was implemented to inteface with ROS, an open-source robotics platform.
We demonstrated that the algorithm is useful for autonomous grasping and also as a grasp suggestion tool for teleoperation (see below).
It was generalized to work with any 3D point cloud sensor on a robot, and we have started integrating it into other parts of the ROS grasping tools.
Details of the software can be found at <a href="http://www.ros.org/wiki/pr2_grasp_adjust">pr2_grasp_adjust</a>.</p>
</td></tr></table>
<h3>Assisted Grasping for Teleoperation</h3>
<table class="imgtable"><tr><td>
<img src="images/concatenate1.png" alt="" width="200px" height="150px" />&nbsp;</td>
<td align="left"><p>We integrated the above technology into a teleoperation system using a PR2 robot from Willow Garage.
The gripper stereo sensor was mounted to the gripper of the PR2, and we enabled joystick-driven arm control from a head- or gripper-camera perspective.
The automatic grasp adjustment algorithm was integrated into this system to allow the teleoperator to move into the vicinity of an object and then execute a grasp with a couple of button presses, saving from the possible embarassment of knocking an object over by clumsy joystick movements.
In a related strategy, the vision system enables control or constraint of some
of the the robot's motions in the vinicinty of an object, thus reducing the number of degrees of
freedom that the operator must control and reducing cognitive load.</p>
</td></tr></table>
<div id="footer">
<div id="footer-text">
Page generated 2019-12-13 11:58:44 PST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
(<a href="index.jemdoc">source</a>)
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
